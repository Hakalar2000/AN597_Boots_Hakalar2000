---
title: "Hakalar_PeerCommentary_gpelose_05"
author: "Greg Pelose"
date: "11/9/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

When we initially discussed the central limit theorem and confidence intervals, we showed how we could use bootstrapping to estimate standard errors and confidence intervals around certain parameter values, like the mean. Using bootstrapping, we could also do the same for estimating standard errors and CIs around regression parameters, such as β
 coefficients.

#[1] Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).

#[2] Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.

##Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.

##How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?

##How does the latter compare to the 95% CI estimated from your entire dataset?

Lets load in our packages
```{r}
install.packages("simpleboot")
install.packages("boot")
install.packages("Rmisc")
install.packages("psych")
```

```{r}
library(curl)
library(simpleboot) #for bootstrapping
library(boot) #for CIs in bootstrapping
library(Rmisc) #for CIs in your dataset
library(psych) #for SE for our dataset
```

Lets load in our data
```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
t <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
t<-na.omit(t) #yet again NAs got problematic...
h<-t$HomeRange_km2
m<-t$Body_mass_female_mean
n<-cbind(h, m)
length(t)
d<-data.frame(n)
CI(h, ci = 0.95)
CI(m, ci = 0.95)
describe(n, type = 2)
```

Lets run a log transformed linear regression 
```{r}
l<-lm(data = t, log(h)~log(m))
l
names(l)
l$coefficients #lets play around with what coefficients we got
l$coefficients[1] 
l$coefficients[2]
summary(l) #gives SE
confint(l, level = 0.90) #gives CI
```
Wonderful
slope: 1.087 (t2)
Intercept: -9.750 (t1)
SE intercept: 1.7750
SE slope: 0.2014

Now lets take 1000 samples of our data (I think we want nonparametric since we are doing samplig with replacement)
```{r}
# Bootstrap 95% CI for regression coefficients 
# function to obtain regression weights 
?sample
s<-sample(h, 1000, replace = TRUE)
s
head(s)
logit.bootstrap <- function(data, indices) {
  d <- data[indices, ]
  fit <- lm(data = d, log(h)~log(m))
  return(coef(fit))
} #This function basically calculated the linear model for each of my outputs. 
set.seed(800) # I just chose something tbh. 
logit.boot <- boot(data=d, statistic=logit.bootstrap, R=1000) # 1000 samples plus I had to turn n into a dataframe which I called d. 
logit.boot #gives us the Standard error
names(logit.boot)
plot(logit.boot)
#intercept
boot.ci(logit.boot, index = 1, conf = 0.95) #index gives us either the slope or the intercept
#slope
boot.ci(logit.boot, index = 2, conf = 0.95)
#not sure what the "boostrap variances needed for studentized intervals" error is...not sure I want to do anything about it either. 
```

The SEs for the boostrap are slightly different. The intercept coefficient has a slightly larger standard error, while the slope is only slightly smaller. 

The linear model itself has CIs that are very close to the boostrap CIs. 

Things I don't understand
#How can we compare CIs or SEs of the actual dataset to the values for the models? They are log transformed?
#didn't even try the extra credit....
#I don't think I based anything on quantiles and I am not sure which type of CI I should use. 

Suggestions
#our last homework should require us to do something GLMs so we are forced to practice. 

